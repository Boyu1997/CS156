{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and find freature present in both approved and rejected data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "approved_csv = pd.read_csv(\"LoanStats3a.csv\", skiprows=1)\n",
    "rejected_csv = pd.read_csv(\"RejectStatsA.csv\", skiprows=1)\n",
    "\n",
    "### Mapping ###\n",
    "# 'loan_amnt' - 'Amount Requested'\n",
    "# 'issue_d' - 'Application Date'\n",
    "# 'purpose', 'title' - 'Loan Title'\n",
    "# '' - 'Risk_Score' (no match)\n",
    "# 'dti' - 'Debt-To-Income Ratio'\n",
    "# 'zip_code' - 'Zip Code'\n",
    "# 'addr_state' - 'State'\n",
    "# 'policy_code' - 'Policy Code'\n",
    "# 'emp_length' - 'Employment Length'\n",
    "approved_raw_data = approved_csv[['loan_amnt','issue_d', 'purpose', 'dti', 'zip_code',\n",
    "                                  'addr_state', 'policy_code','emp_length']]\n",
    "rejected_raw_data = rejected_csv[['Amount Requested','Application Date', 'Loan Title',\n",
    "                                  'Debt-To-Income Ratio', 'Zip Code', 'State',\n",
    "                                  'Policy Code', 'Employment Length']]\n",
    "rejected_raw_data.columns = ['loan_amnt','issue_d', 'purpose', 'dti', 'zip_code',\n",
    "                             'addr_state', 'policy_code', 'emp_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of entries avaliable in each set, and display a small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contain 42538 approved data.\n",
      "The dataset contain 755491 rejected data.\n",
      "\n",
      "\n",
      "Sample from approved data:\n",
      "       loan_amnt   issue_d             purpose    dti zip_code addr_state  \\\n",
      "31404     6250.0  Mar-2010  debt_consolidation  18.04    342xx         FL   \n",
      "25328     5000.0  Sep-2010  debt_consolidation  16.78    010xx         MA   \n",
      "11346     3050.0  Jul-2011  debt_consolidation  16.65    799xx         TX   \n",
      "21971    11500.0  Dec-2010  debt_consolidation  15.32    923xx         CA   \n",
      "32074     8000.0  Feb-2010      small_business   1.65    770xx         TX   \n",
      "\n",
      "       policy_code emp_length  \n",
      "31404          1.0  10+ years  \n",
      "25328          1.0    8 years  \n",
      "11346          1.0   < 1 year  \n",
      "21971          1.0    5 years  \n",
      "32074          1.0     1 year  \n",
      "\n",
      "\n",
      "Sample from rejected data:\n",
      "        loan_amnt     issue_d             purpose     dti zip_code addr_state  \\\n",
      "3670      25000.0  2007-12-02           2Elisaiah   34.2%    300xx         GA   \n",
      "748504    25000.0  2012-12-26         credit_card  10.18%    761xx         TX   \n",
      "557466    25000.0  2012-07-07  debt_consolidation  19.41%    785xx         TX   \n",
      "336713     3000.0  2011-09-03                 car   51.8%    352xx         AL   \n",
      "83793     10000.0  2009-12-01                          0%    412xx         KY   \n",
      "\n",
      "        policy_code emp_length  \n",
      "3670              0    5 years  \n",
      "748504            0   < 1 year  \n",
      "557466            0   < 1 year  \n",
      "336713            0   < 1 year  \n",
      "83793             0  10+ years  \n"
     ]
    }
   ],
   "source": [
    "print (\"The dataset contain {:d} approved data.\".format(approved_raw_data.shape[0]))\n",
    "print (\"The dataset contain {:d} rejected data.\".format(rejected_raw_data.shape[0]))\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"Sample from approved data:\")\n",
    "print (approved_raw_data.sample(5))\n",
    "print (\"\\n\")\n",
    "print (\"Sample from rejected data:\")\n",
    "print (rejected_raw_data.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how may entries have missing value for both set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             approved  approved %  rejected  rejected %\n",
      "loan_amnt           3    0.000071         0    0.000000\n",
      "issue_d             3    0.000071         0    0.000000\n",
      "purpose             3    0.000071        14    0.000019\n",
      "dti                 3    0.000071         0    0.000000\n",
      "zip_code            3    0.000071        22    0.000029\n",
      "addr_state          3    0.000071        21    0.000028\n",
      "policy_code         3    0.000071         0    0.000000\n",
      "emp_length       1115    0.026212      8130    0.010761\n"
     ]
    }
   ],
   "source": [
    "nan_count = pd.DataFrame({\"approved\": approved_raw_data.isnull().sum(),\n",
    "                          \"approved %\": approved_raw_data.isnull().sum() / len(approved_raw_data), \n",
    "                          \"rejected\": rejected_raw_data.isnull().sum(),\n",
    "                          \"rejected %\": rejected_raw_data.isnull().sum() / len(rejected_raw_data)})\n",
    "print (nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, the dataset is relatively complet, with most of the missing value occur in the 'emp_length' feature. However, it is still less than 3% of missing value for the approved set and about 1% for the rejected set. This means it is relatively safe to simply drop the entries with NaN value with low possibility of introducing a significant bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_raw_data = approved_raw_data.dropna()\n",
    "rejected_raw_data = rejected_raw_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 'issue_d' and 'dti' are coded in different format in the approved and rejected set. We will need to standardize the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# deep copy raw data into intermediate processing set 0\n",
    "approved_data_0 = copy.deepcopy(approved_raw_data)\n",
    "rejected_data_0 = copy.deepcopy(rejected_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def date_string_to_datetime(row):\n",
    "    try:   # for approved dataset\n",
    "        return datetime.strptime(row['issue_d'], \"%b-%Y\")\n",
    "    except:   # for rejected dataset\n",
    "        return datetime.strptime(row['issue_d'], \"%Y-%m-%d\")\n",
    "\n",
    "def extract_year_lambda(row):\n",
    "    return date_string_to_datetime(row).year\n",
    "\n",
    "def extract_month_lambda(row):\n",
    "    return date_string_to_datetime(row).month\n",
    "\n",
    "def dti_percent_to_decimal(row):\n",
    "    return np.float64(row['dti'].strip(\"%\"))\n",
    "    \n",
    "\n",
    "approved_data_0['issue_y'] = approved_data_0.apply(extract_year_lambda, axis=1)\n",
    "approved_data_0['issue_m'] = approved_data_0.apply(extract_month_lambda, axis=1)\n",
    "approved_data_0 = approved_data_0.drop(columns=['issue_d'])\n",
    "\n",
    "rejected_data_0['issue_y'] = rejected_data_0.apply(extract_year_lambda, axis=1)\n",
    "rejected_data_0['issue_m'] = rejected_data_0.apply(extract_month_lambda, axis=1)\n",
    "rejected_data_0['dti'] = rejected_data_0.apply(dti_percent_to_decimal, axis=1)\n",
    "rejected_data_0 = rejected_data_0.drop(columns=['issue_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from approved data:\n",
      "       loan_amnt             purpose    dti zip_code addr_state  policy_code  \\\n",
      "34453    25000.0             wedding   9.96    070xx         NJ          1.0   \n",
      "39199    15000.0  debt_consolidation  15.10    275xx         NC          1.0   \n",
      "40935    15000.0  debt_consolidation  20.44    334xx         FL          1.0   \n",
      "28389    14000.0  debt_consolidation  20.98    750xx         TX          1.0   \n",
      "17704     2400.0      major_purchase  18.47    749xx         OK          1.0   \n",
      "\n",
      "      emp_length  issue_y  issue_m  \n",
      "34453    4 years     2009       11  \n",
      "39199   < 1 year     2008        3  \n",
      "40935    8 years     2009       10  \n",
      "28389    2 years     2010        7  \n",
      "17704  10+ years     2011        3  \n",
      "\n",
      "\n",
      "Sample from rejected data:\n",
      "        loan_amnt                  purpose    dti zip_code addr_state  \\\n",
      "414756     3000.0                    other   6.79    583xx         MN   \n",
      "209994    25000.0           small_business   2.67    786xx         TX   \n",
      "390181     3000.0                      car   0.00    300xx         GA   \n",
      "503258    25000.0  Debt Consolidation Loan  14.76    303xx         GA   \n",
      "156202    25000.0       debt_consolidation  83.47    127xx         NY   \n",
      "\n",
      "        policy_code emp_length  issue_y  issue_m  \n",
      "414756            0   < 1 year     2011       12  \n",
      "209994            0   < 1 year     2011        1  \n",
      "390181            0   < 1 year     2011       11  \n",
      "503258            0    9 years     2012        5  \n",
      "156202            0   < 1 year     2010        9  \n"
     ]
    }
   ],
   "source": [
    "print (\"Sample from approved data:\")\n",
    "print (approved_data_0.sample(5))\n",
    "print (\"\\n\")\n",
    "print (\"Sample from rejected data:\")\n",
    "print (rejected_data_0.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 'purpose' is a string, containing discription of the intened use of the loan. We need to find a way to represent that data in a machine-friendly way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'purpose' feature count in approved dataset:\n",
      "                    policy_code\n",
      "purpose                        \n",
      "car                        1563\n",
      "credit_card                5344\n",
      "debt_consolidation        19363\n",
      "educational                 413\n",
      "home_improvement           3099\n",
      "house                       412\n",
      "major_purchase             2238\n",
      "medical                     726\n",
      "moving                      603\n",
      "other                      4259\n",
      "renewable_energy             98\n",
      "small_business             1946\n",
      "vacation                    368\n",
      "wedding                     991\n",
      "\n",
      "\n",
      "Sample of 'purpose' feature count in rejected dataset:\n",
      "                                          policy_code\n",
      "purpose                                              \n",
      "Tradition pay-off                                   1\n",
      "Business Supplies                                   1\n",
      "Loan for student loand and paying bills             1\n",
      "BE ABLE TO BE BACK ON TRACK.............            1\n",
      "For College                                         2\n",
      "terrylou                                            1\n",
      "Cadillac 96                                         1\n",
      "Need Quick-In Escrow                                1\n",
      "Replace Privacy Fence and Update Kitchen            1\n",
      "looking to expand                                   1\n"
     ]
    }
   ],
   "source": [
    "# in the approved set, the purpose data is well organized label\n",
    "print (\"'purpose' feature count in approved dataset:\")\n",
    "print (approved_data_0[['purpose', 'policy_code']].groupby('purpose').count())\n",
    "print (\"\\n\")\n",
    "\n",
    "# in the rejected set, the purpose data is messy text string\n",
    "print (\"Sample of 'purpose' feature count in rejected dataset:\")\n",
    "print (rejected_data_0[['purpose', 'policy_code']].groupby('purpose').count().sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To clean the 'purpose' feature data in the rejected set, we will use KNN to cluster the text strings in rejected set to the labels in approved set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms in each cluster:\n",
      "Cluster 0: moving, loan, expenses\n",
      "Cluster 1: debt_consolidation, factoring, facelift\n",
      "Cluster 2: home_improvement, loan, zzzzzgirl\n",
      "Cluster 3: credit_card, loan, zzzzzgirl\n",
      "Cluster 4: loan, consolidation, debt\n",
      "Cluster 5: major_purchase, loan, zzzzzgirl\n",
      "Cluster 6: medical, expenses, loan\n",
      "Cluster 7: car, loan, need\n",
      "Cluster 8: small_business, loan, zzzzzgirl\n",
      "Cluster 9: house, loan, buy\n",
      "Cluster 10: wedding, loan, expenses\n",
      "Cluster 11: vacation, loan, dream\n",
      "Cluster 12: consolidate, debt, credit\n",
      "Cluster 13: bills, pay, medical\n",
      "\n",
      "\n",
      "Predicted mapping between approved set 'purpose' label and rejected set 'purpose' text text\n",
      "        original_label  predicted_cluster predicted_cluster_top_label\n",
      "0                  car                  7                         car\n",
      "1          credit_card                  3                 credit_card\n",
      "2   debt_consolidation                  1          debt_consolidation\n",
      "3          educational                  4                        loan\n",
      "4     home_improvement                  2            home_improvement\n",
      "5                house                  9                       house\n",
      "6       major_purchase                  5              major_purchase\n",
      "7              medical                  6                     medical\n",
      "8               moving                  0                      moving\n",
      "9                other                  4                        loan\n",
      "10    renewable_energy                  4                        loan\n",
      "11      small_business                  8              small_business\n",
      "12            vacation                 11                    vacation\n",
      "13             wedding                 10                     wedding\n",
      "Unmatched clusters: [12, 13]\n",
      "With the top label in cluster being: ['consolidate', 'bills']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "# extract 'purpose' feature data for both sets\n",
    "rejected_purpose_text = list(rejected_data_0['purpose'])\n",
    "approved_purpose_label = list(approved_data_0.groupby('purpose').groups.keys())\n",
    "\n",
    "# build knn cluster model using all text in the rejected set\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(rejected_purpose_text)\n",
    "true_k = 14\n",
    "knn_cluster_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "knn_cluster_model.fit(X)\n",
    "\n",
    "# print a summary of the clusters\n",
    "print (\"Top terms in each cluster:\")\n",
    "order_centroids = knn_cluster_model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range (true_k):\n",
    "    print (\"Cluster {:d}: {:s}, {:s}, {:s}\".format(* [i] + [terms[ind] for ind in order_centroids[i, :3]] ))\n",
    "\n",
    "# map the approved set label to the cluster model using predict methord\n",
    "Y = vectorizer.transform(approved_purpose_label)\n",
    "prediction = knn_cluster_model.predict(Y)\n",
    "label_mapping = pd.DataFrame({'original_label': approved_purpose_label, 'predicted_cluster': prediction,\n",
    "                              'predicted_cluster_top_label': [terms[order_centroids[c][0]] for c in prediction]})\n",
    "print (\"\\n\")\n",
    "print (\"Predicted mapping between approved set 'purpose' label and rejected set 'purpose' text text\")\n",
    "print (label_mapping)\n",
    "unmatched_cluster = list(set(range(14)) - set(prediction))\n",
    "print (\"Unmatched clusters:\", unmatched_cluster)\n",
    "print (\"With the top label in cluster being:\", [terms[order_centroids[c][0]] for c in unmatched_cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By applying KNN clustering on the rejected set, we are able to map most of the label in the approved set to the cluster of the rejected set. The exceptions we have  are for the 'educational', 'other', and 'renewable_energy' label. However, as 'educational' and 'renewable_energy' are relatively small group (413 and 98 entries compare to the over 40k dataset) and 'other' simply means other, we can group these labels together as 'other'. For the unmatched cluster 12 and 13, its top keyword being 'consolidate' and 'bills' also fit to the description of 'other'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# deep copy intermediate processing set 0 into intermediate processing set 1\n",
    "approved_data_1 = copy.deepcopy(approved_data_0)\n",
    "rejected_data_1 = copy.deepcopy(rejected_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 41423/41423 [00:22<00:00, 1829.03it/s]\n",
      "Pandas Apply:  95%|█████████▍| 708866/747325 [07:06<00:27, 1389.55it/s]"
     ]
    }
   ],
   "source": [
    "import swifter \n",
    "\n",
    "def text_to_cluster_code(row):\n",
    "    text = vectorizer.transform([row['purpose']])\n",
    "    cluster_code = knn_cluster_model.predict(text)[0]\n",
    "    #cluster_code = cluster_code[0]\n",
    "    if cluster_code is 12 or cluster_code is 13:\n",
    "        cluster_code = 4\n",
    "    return cluster_code\n",
    "\n",
    "approved_data_1['purpose'] = approved_data_1.swifter.apply(text_to_cluster_code, axis=1)\n",
    "rejected_data_1['purpose'] = rejected_data_1.swifter.apply(text_to_cluster_code, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Sample from approved data:\")\n",
    "print (approved_data_1.sample(5))\n",
    "print (\"\\n\")\n",
    "print (\"Sample from rejected data:\")\n",
    "print (rejected_data_1.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 'emp_length' now is a categorical data, however, as the nature of the data is numerical, we will convert the it into numerical data. Feature 'purpose', 'addr_state', 'issue_y', and 'issue_m' are categorical data, and we will represent them using dummy variables. Feature 'zip_code' is between a categorical and numerical data, and is very difficuly to use directly as a feature without more pre-processing. We will drop it for simplicity purpose. In addition, we will normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# deep copy intermediate processing set 1 into intermediate processing set 2\n",
    "approved_data_2 = copy.deepcopy(approved_data_1)\n",
    "rejected_data_2 = copy.deepcopy(rejected_data_1)\n",
    "\n",
    "# at this point, the remaining job is only to clean up, we can join the two sets\n",
    "# policy_code=1 represent approved, policy_code=0 represent rejected\n",
    "data = pd.concat([approved_data_2, rejected_data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Convert originally categorical data employment length to numerical data\n",
    "data['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True,\n",
    "                                regex=True)\n",
    "data['emp_length'] = data['emp_length'].astype(int)\n",
    "\n",
    "# generate dummy variables for 'purpose', 'addr_state', 'issue_y', and 'issue_m'\n",
    "data = pd.get_dummies(data, columns=['purpose', 'addr_state', 'issue_y', 'issue_m'])\n",
    "\n",
    "# normalize the data\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled = min_max_scaler.fit_transform(df.values)\n",
    "df = pandas.DataFrame(scaled)\n",
    "\n",
    "print (data.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:, df.columns != 'policy_code']\n",
    "X = X.values\n",
    "Y = data['policy_code']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit training data into a logistic classification model\n",
    "logistic_predict_model = LogisticRegression()\n",
    "logistic_predict_model.fit(X_train, Y_train[:,1])\n",
    "\n",
    "# predict on the test set data\n",
    "Y_pred = reg.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test[:,1], Y_pred)\n",
    "print ('The accuracy score is %.3f' %accuracy)\n",
    "scores = cross_val_score(reg, X_train, Y_train[:,1], cv=5) print('the cross validation score is %.3f' %np.mean(scores)) print(classification_report(Y_test[:,1], Y_pred))\n",
    "         # The coefficients\n",
    "         print('Coefficients:',reg.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
