{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and find freature present in both approved and rejected data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "approved_csv = pd.read_csv(\"LoanStats3a.csv\", skiprows=1)\n",
    "rejected_csv = pd.read_csv(\"RejectStatsA.csv\", skiprows=1)\n",
    "\n",
    "### Mapping ###\n",
    "# 'loan_amnt' - 'Amount Requested'\n",
    "# 'issue_d' - 'Application Date'\n",
    "# 'purpose', 'title' - 'Loan Title'\n",
    "# '' - 'Risk_Score' (no match)\n",
    "# 'dti' - 'Debt-To-Income Ratio'\n",
    "# 'zip_code' - 'Zip Code'\n",
    "# 'addr_state' - 'State'\n",
    "# 'policy_code' - 'Policy Code'\n",
    "# 'emp_length' - 'Employment Length'\n",
    "approved_raw_data = approved_csv[['loan_amnt','issue_d', 'purpose', 'dti', 'zip_code',\n",
    "                                  'addr_state', 'policy_code','emp_length']]\n",
    "rejected_raw_data = rejected_csv[['Amount Requested','Application Date', 'Loan Title',\n",
    "                                  'Debt-To-Income Ratio', 'Zip Code', 'State',\n",
    "                                  'Policy Code', 'Employment Length']]\n",
    "rejected_raw_data.columns = ['loan_amnt','issue_d', 'purpose', 'dti', 'zip_code',\n",
    "                             'addr_state', 'policy_code', 'emp_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of entries avaliable in each set, and display a small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contain 42538 approved data.\n",
      "The dataset contain 755491 rejected data.\n",
      "\n",
      "\n",
      "Sample from approved data:\n",
      "       loan_amnt   issue_d             purpose    dti zip_code addr_state  \\\n",
      "11694     9000.0  Jul-2011             wedding  17.70    900xx         CA   \n",
      "17019    10000.0  Apr-2011  debt_consolidation  17.58    131xx         NY   \n",
      "27972     3000.0  Jul-2010  debt_consolidation   7.67    212xx         MD   \n",
      "20105     5000.0  Feb-2011         credit_card  15.53    029xx         RI   \n",
      "19205    21600.0  Feb-2011                 car   7.62    070xx         NJ   \n",
      "\n",
      "       policy_code emp_length  \n",
      "11694          1.0    2 years  \n",
      "17019          1.0    6 years  \n",
      "27972          1.0    6 years  \n",
      "20105          1.0  10+ years  \n",
      "19205          1.0    7 years  \n",
      "\n",
      "\n",
      "Sample from rejected data:\n",
      "        loan_amnt     issue_d             purpose     dti zip_code addr_state  \\\n",
      "172235    20000.0  2010-10-18  debt_consolidation   55.9%    559xx         MN   \n",
      "546388    10000.0  2012-06-24      major_purchase  46.16%    775xx         TX   \n",
      "408436    10000.0  2011-12-12               other   29.5%    913xx         CA   \n",
      "732410     1000.0  2012-12-11  debt_consolidation      0%    970xx         OR   \n",
      "529921     6000.0  2012-06-05         credit_card  53.82%    191xx         PA   \n",
      "\n",
      "        policy_code emp_length  \n",
      "172235            0   < 1 year  \n",
      "546388            0   < 1 year  \n",
      "408436            0   < 1 year  \n",
      "732410            0   < 1 year  \n",
      "529921            0   < 1 year  \n"
     ]
    }
   ],
   "source": [
    "print (\"The dataset contain {:d} approved data.\".format(approved_raw_data.shape[0]))\n",
    "print (\"The dataset contain {:d} rejected data.\".format(rejected_raw_data.shape[0]))\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"Sample from approved data:\")\n",
    "print (approved_raw_data.sample(5))\n",
    "print (\"\\n\")\n",
    "print (\"Sample from rejected data:\")\n",
    "print (rejected_raw_data.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how may entries have missing value for both set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             approved  approved %  rejected  rejected %\n",
      "loan_amnt           3    0.000071         0    0.000000\n",
      "issue_d             3    0.000071         0    0.000000\n",
      "purpose             3    0.000071        14    0.000019\n",
      "dti                 3    0.000071         0    0.000000\n",
      "zip_code            3    0.000071        22    0.000029\n",
      "addr_state          3    0.000071        21    0.000028\n",
      "policy_code         3    0.000071         0    0.000000\n",
      "emp_length       1115    0.026212      8130    0.010761\n"
     ]
    }
   ],
   "source": [
    "nan_count = pd.DataFrame({\"approved\": approved_raw_data.isnull().sum(),\n",
    "                          \"approved %\": approved_raw_data.isnull().sum() / len(approved_raw_data), \n",
    "                          \"rejected\": rejected_raw_data.isnull().sum(),\n",
    "                          \"rejected %\": rejected_raw_data.isnull().sum() / len(rejected_raw_data)})\n",
    "print (nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, the dataset is relatively complet, with most of the missing value occur in the 'emp_length' feature. However, it is still less than 3% of missing value for the approved set and about 1% for the rejected set. This means it is relatively safe to simply drop the entries with NaN value with low possibility of introducing a significant bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_raw_data = approved_raw_data.dropna()\n",
    "rejected_raw_data = rejected_raw_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 'issue_d' and 'dti' are coded in different format in the approved and rejected set. We will need to standardize the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# deep copy raw data into intermediate processing set 0\n",
    "approved_data_0 = copy.deepcopy(approved_raw_data)\n",
    "rejected_data_0 = copy.deepcopy(rejected_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def date_string_to_datetime(row):\n",
    "    try:   # for approved dataset\n",
    "        return datetime.strptime(row['issue_d'], \"%b-%Y\")\n",
    "    except:   # for rejected dataset\n",
    "        return datetime.strptime(row['issue_d'], \"%Y-%m-%d\")\n",
    "\n",
    "def extract_year_lambda(row):\n",
    "    return date_string_to_datetime(row).year\n",
    "\n",
    "def extract_month_lambda(row):\n",
    "    return date_string_to_datetime(row).month\n",
    "\n",
    "def dti_percent_to_decimal(row):\n",
    "    return np.float64(row['dti'].strip(\"%\"))\n",
    "    \n",
    "\n",
    "approved_data_0['issue_y'] = approved_data_0.apply(extract_year_lambda, axis=1)\n",
    "approved_data_0['issue_m'] = approved_data_0.apply(extract_month_lambda, axis=1)\n",
    "approved_data_0 = approved_data_0.drop(columns=['issue_d'])\n",
    "\n",
    "rejected_data_0['issue_y'] = rejected_data_0.apply(extract_year_lambda, axis=1)\n",
    "rejected_data_0['issue_m'] = rejected_data_0.apply(extract_month_lambda, axis=1)\n",
    "rejected_data_0['dti'] = rejected_data_0.apply(dti_percent_to_decimal, axis=1)\n",
    "rejected_data_0 = rejected_data_0.drop(columns=['issue_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from approved data:\n",
      "       loan_amnt             purpose    dti zip_code addr_state  policy_code  \\\n",
      "38685     4500.0  debt_consolidation   7.53    210xx         MD          1.0   \n",
      "26020     4000.0  debt_consolidation  13.88    112xx         NY          1.0   \n",
      "36852    12000.0         credit_card   6.58    539xx         WI          1.0   \n",
      "28523     5000.0               other  14.80    926xx         CA          1.0   \n",
      "38772     6000.0  debt_consolidation  18.99    940xx         CA          1.0   \n",
      "\n",
      "      emp_length  issue_y  issue_m  \n",
      "38685  10+ years     2008        6  \n",
      "26020   < 1 year     2010        9  \n",
      "36852    9 years     2009        5  \n",
      "28523    3 years     2010        6  \n",
      "38772    4 years     2008        5  \n",
      "\n",
      "\n",
      "Sample from rejected data:\n",
      "        loan_amnt            purpose    dti zip_code addr_state  policy_code  \\\n",
      "473074    30000.0  Freedom from Debt  22.40    982xx         WA            0   \n",
      "45830     25000.0                 bn   9.84    331xx         FL            0   \n",
      "143538     2000.0              other   2.22    360xx         AL            0   \n",
      "668090    10000.0              house   0.00    925xx         CA            0   \n",
      "173634    10000.0              other  23.85    303xx         GA            0   \n",
      "\n",
      "       emp_length  issue_y  issue_m  \n",
      "473074  10+ years     2012        3  \n",
      "45830    < 1 year     2009        4  \n",
      "143538    3 years     2010        8  \n",
      "668090   < 1 year     2012       10  \n",
      "173634   < 1 year     2010       10  \n"
     ]
    }
   ],
   "source": [
    "print (\"Sample from approved data:\")\n",
    "print (approved_data_0.sample(5))\n",
    "print (\"\\n\")\n",
    "print (\"Sample from rejected data:\")\n",
    "print (rejected_data_0.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 'purpose' is a string, containing discription of the intened use of the loan. We need to find a way to represent that data in a machine-friendly way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'purpose' feature count in approved dataset:\n",
      "                    policy_code\n",
      "purpose                        \n",
      "car                        1563\n",
      "credit_card                5344\n",
      "debt_consolidation        19363\n",
      "educational                 413\n",
      "home_improvement           3099\n",
      "house                       412\n",
      "major_purchase             2238\n",
      "medical                     726\n",
      "moving                      603\n",
      "other                      4259\n",
      "renewable_energy             98\n",
      "small_business             1946\n",
      "vacation                    368\n",
      "wedding                     991\n",
      "\n",
      "\n",
      "Sample of 'purpose' feature count in rejected dataset:\n",
      "                            policy_code\n",
      "purpose                                \n",
      "Help me secure my future.             1\n",
      "helping me                            1\n",
      "Freightliner                          1\n",
      "Home loan needed                      1\n",
      "small business help                   1\n",
      "Retirement credit free                1\n",
      "A Home Staging Degree                 1\n",
      "Facebook IPO                          1\n",
      "Recently moved ...                    1\n",
      "Great Debt to Income Ratio            1\n"
     ]
    }
   ],
   "source": [
    "# in the approved set, the purpose data is well organized label\n",
    "print (\"'purpose' feature count in approved dataset:\")\n",
    "print (approved_data_0[['purpose', 'policy_code']].groupby('purpose').count())\n",
    "print (\"\\n\")\n",
    "\n",
    "# in the rejected set, the purpose data is messy text string\n",
    "print (\"Sample of 'purpose' feature count in rejected dataset:\")\n",
    "print (rejected_data_0[['purpose', 'policy_code']].groupby('purpose').count().sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To clean the 'purpose' feature data in the rejected set, we will use KNN to cluster the text strings in rejected set to the labels in approved set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms in each cluster:\n",
      "Cluster 0: moving, loan, expenses\n",
      "Cluster 1: debt_consolidation, factoring, facelift\n",
      "Cluster 2: home_improvement, loan, zzzzzgirl\n",
      "Cluster 3: credit_card, loan, zzzzzgirl\n",
      "Cluster 4: loan, consolidation, debt\n",
      "Cluster 5: major_purchase, loan, zzzzzgirl\n",
      "Cluster 6: medical, expenses, loan\n",
      "Cluster 7: car, loan, need\n",
      "Cluster 8: small_business, loan, zzzzzgirl\n",
      "Cluster 9: house, loan, buy\n",
      "Cluster 10: wedding, loan, expenses\n",
      "Cluster 11: vacation, loan, dream\n",
      "Cluster 12: consolidate, debt, credit\n",
      "Cluster 13: bills, pay, medical\n",
      "\n",
      "\n",
      "Predicted mapping between approved set 'purpose' label and rejected set 'purpose' text text\n",
      "        original_label  predicted_cluster predicted_cluster_top_label\n",
      "0                  car                  7                         car\n",
      "1          credit_card                  3                 credit_card\n",
      "2   debt_consolidation                  1          debt_consolidation\n",
      "3          educational                  4                        loan\n",
      "4     home_improvement                  2            home_improvement\n",
      "5                house                  9                       house\n",
      "6       major_purchase                  5              major_purchase\n",
      "7              medical                  6                     medical\n",
      "8               moving                  0                      moving\n",
      "9                other                  4                        loan\n",
      "10    renewable_energy                  4                        loan\n",
      "11      small_business                  8              small_business\n",
      "12            vacation                 11                    vacation\n",
      "13             wedding                 10                     wedding\n",
      "Unmatched clusters: [12, 13]\n",
      "With the top label in cluster being: ['consolidate', 'bills']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "# extract 'purpose' feature data for both sets\n",
    "rejected_purpose_text = list(rejected_data_0['purpose'])\n",
    "approved_purpose_label = list(approved_data_0.groupby('purpose').groups.keys())\n",
    "\n",
    "# build knn cluster model using all text in the rejected set\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(rejected_purpose_text)\n",
    "true_k = 14\n",
    "knn_cluster_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "knn_cluster_model.fit(X)\n",
    "\n",
    "# print a summary of the clusters\n",
    "print (\"Top terms in each cluster:\")\n",
    "order_centroids = knn_cluster_model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range (true_k):\n",
    "    print (\"Cluster {:d}: {:s}, {:s}, {:s}\".format(* [i] + [terms[ind] for ind in order_centroids[i, :3]] ))\n",
    "\n",
    "# map the approved set label to the cluster model using predict methord\n",
    "Y = vectorizer.transform(approved_purpose_label)\n",
    "prediction = knn_cluster_model.predict(Y)\n",
    "label_mapping = pd.DataFrame({'original_label': approved_purpose_label, 'predicted_cluster': prediction,\n",
    "                              'predicted_cluster_top_label': [terms[order_centroids[c][0]] for c in prediction]})\n",
    "print (\"\\n\")\n",
    "print (\"Predicted mapping between approved set 'purpose' label and rejected set 'purpose' text text\")\n",
    "print (label_mapping)\n",
    "unmatched_cluster = list(set(range(14)) - set(prediction))\n",
    "print (\"Unmatched clusters:\", unmatched_cluster)\n",
    "print (\"With the top label in cluster being:\", [terms[order_centroids[c][0]] for c in unmatched_cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By applying KNN clustering on the rejected set, we are able to map most of the label in the approved set to the cluster of the rejected set. The exceptions we have  are for the 'educational', 'other', and 'renewable_energy' label. However, as 'educational' and 'renewable_energy' are relatively small group (413 and 98 entries compare to the over 40k dataset) and 'other' simply means other, we can group these labels together as 'other'. For the unmatched cluster 12 and 13, its top keyword being 'consolidate' and 'bills' also fit to the description of 'other'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# deep copy intermediate processing set 0 into intermediate processing set 1\n",
    "approved_data_1 = copy.deepcopy(approved_data_0)\n",
    "rejected_data_1 = copy.deepcopy(rejected_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 41423/41423 [00:24<00:00, 1664.16it/s]\n",
      "Pandas Apply: 100%|██████████| 747325/747325 [06:53<00:00, 1805.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import swifter \n",
    "\n",
    "def text_to_cluster_code(row):\n",
    "    text = vectorizer.transform([row['purpose']])\n",
    "    cluster_code = knn_cluster_model.predict(text)[0]\n",
    "    #cluster_code = cluster_code[0]\n",
    "    if cluster_code is 12 or cluster_code is 13:\n",
    "        cluster_code = 4\n",
    "    return cluster_code\n",
    "\n",
    "approved_data_1['purpose'] = approved_data_1.swifter.apply(text_to_cluster_code, axis=1)\n",
    "rejected_data_1['purpose'] = rejected_data_1.swifter.apply(text_to_cluster_code, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from approved data:\n",
      "       loan_amnt  purpose    dti zip_code addr_state  policy_code emp_length  \\\n",
      "37835     5650.0        1   6.18    927xx         CA          1.0  10+ years   \n",
      "31779     5000.0        1   8.93    107xx         NY          1.0  10+ years   \n",
      "23927    20000.0        2  11.09    189xx         PA          1.0    5 years   \n",
      "3904      6000.0       11  10.98    606xx         IL          1.0     1 year   \n",
      "17841     6000.0        8  13.06    487xx         MI          1.0     1 year   \n",
      "\n",
      "       issue_y  issue_m  \n",
      "37835     2009        1  \n",
      "31779     2010        3  \n",
      "23927     2010       11  \n",
      "3904      2011       11  \n",
      "17841     2011        4  \n",
      "\n",
      "\n",
      "Sample from rejected data:\n",
      "        loan_amnt  purpose    dti zip_code addr_state  policy_code emp_length  \\\n",
      "518432    15000.0        1  14.00    280xx         NC            0    8 years   \n",
      "141699    30000.0        1  39.45    154xx         PA            0  10+ years   \n",
      "495626    25000.0        5   5.78    212xx         MD            0   < 1 year   \n",
      "629279     5000.0        1   7.87    549xx         WI            0   < 1 year   \n",
      "737255    25000.0        4  67.79    553xx         MN            0   < 1 year   \n",
      "\n",
      "        issue_y  issue_m  \n",
      "518432     2012        5  \n",
      "141699     2010        7  \n",
      "495626     2012        4  \n",
      "629279     2012        9  \n",
      "737255     2012       12  \n"
     ]
    }
   ],
   "source": [
    "print (\"Sample from approved data:\")\n",
    "print (approved_data_1.sample(5))\n",
    "print (\"\\n\")\n",
    "print (\"Sample from rejected data:\")\n",
    "print (rejected_data_1.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 'emp_length' now is a categorical data, however, as the nature of the data is numerical, we will convert the it into numerical data. Feature 'purpose', 'addr_state', 'issue_y', and 'issue_m' are categorical data, and we will represent them using dummy variables. Feature 'zip_code' is between a categorical and numerical data, and is very difficuly to use directly as a feature without more pre-processing. We will drop it for simplicity purpose. In addition, we will normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# deep copy intermediate processing set 1 into intermediate processing set 2\n",
    "approved_data_2 = copy.deepcopy(approved_data_1)\n",
    "rejected_data_2 = copy.deepcopy(rejected_data_1)\n",
    "\n",
    "# at this point, the remaining job is only to clean up, we can join the two sets\n",
    "# policy_code=1 represent approved, policy_code=0 represent rejected\n",
    "data = pd.concat([approved_data_2, rejected_data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loan_amnt           dti  policy_code  emp_length  purpose_0  \\\n",
      "113262   0.010361  1.999999e-08          0.0    0.222222        0.0   \n",
      "206781   0.005359  4.257997e-07          0.0    0.444444        0.0   \n",
      "260393   0.010361  3.999977e-03          0.0    0.000000        0.0   \n",
      "614980   0.019650  1.063799e-06          0.0    0.000000        0.0   \n",
      "483826   0.002108  5.759996e-07          0.0    0.222222        0.0   \n",
      "\n",
      "        purpose_1  purpose_2  purpose_3  purpose_4  purpose_5     ...      \\\n",
      "113262        0.0        0.0        0.0        1.0        0.0     ...       \n",
      "206781        0.0        1.0        0.0        0.0        0.0     ...       \n",
      "260393        0.0        0.0        0.0        0.0        0.0     ...       \n",
      "614980        1.0        0.0        0.0        0.0        0.0     ...       \n",
      "483826        0.0        0.0        0.0        0.0        0.0     ...       \n",
      "\n",
      "        issue_m_3  issue_m_4  issue_m_5  issue_m_6  issue_m_7  issue_m_8  \\\n",
      "113262        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "206781        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "260393        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "614980        0.0        0.0        0.0        0.0        1.0        0.0   \n",
      "483826        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "\n",
      "        issue_m_9  issue_m_10  issue_m_11  issue_m_12  \n",
      "113262        1.0         0.0         0.0         0.0  \n",
      "206781        0.0         1.0         0.0         0.0  \n",
      "260393        0.0         0.0         0.0         0.0  \n",
      "614980        0.0         0.0         0.0         0.0  \n",
      "483826        0.0         0.0         0.0         0.0  \n",
      "\n",
      "[5 rows x 87 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Convert originally categorical data employment length to numerical data\n",
    "data['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True,\n",
    "                                regex=True)\n",
    "data['emp_length'] = data['emp_length'].astype(int)\n",
    "\n",
    "# generate dummy variables for 'purpose', 'addr_state', 'issue_y', and 'issue_m'\n",
    "data = pd.get_dummies(data, columns=['purpose', 'addr_state', 'issue_y', 'issue_m'])\n",
    "\n",
    "# drop 'zip_code' feature\n",
    "data = data.drop(columns=['zip_code'])\n",
    "\n",
    "# normalize the data\n",
    "column_name = list(data.columns.values)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled = min_max_scaler.fit_transform(data.values)\n",
    "data = pd.DataFrame(scaled)\n",
    "data.columns = column_name\n",
    "\n",
    "print (data.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As this is a classification problem, let's start with a logistics regression model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = data['policy_code']\n",
    "data = data.drop(columns=['policy_code'])\n",
    "X = data.values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is 0.952\n",
      "\n",
      "\n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.99      0.97    186823\n",
      "        1.0       0.60      0.24      0.34     10364\n",
      "\n",
      "avg / total       0.94      0.95      0.94    197187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# fit training data into a logistic classification model\n",
    "logistic_predict_model = LogisticRegression()\n",
    "logistic_predict_model.fit(X_train, Y_train)\n",
    "\n",
    "# predict on the test set data\n",
    "Y_predicted = logistic_predict_model.predict(X_test)\n",
    "print (\"The accuracy score is {:.3f}\".format(accuracy_score(Y_test, Y_predicted)))\n",
    "print (\"\\n\")\n",
    "print (\"Classification report:\")\n",
    "print(classification_report(Y_test, Y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use expectation-maximization algorithm to give us more helpful informations. Assume the approved and rejected data set are sample from two different Gaussian distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
