{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Concepts\n",
    "1. MAE: It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight. but should be interpreted cautiously because positive and negative errors will cancel out.\n",
    "2. MSE measures the average magnitude of the error. It’s the average of squared differences between prediction and actual observation.\n",
    "3. the median absolute error is useful basically it is essentially insensitive to outliers (as long as there aren't too many of them). This is because it is the median of all of the absolute values of the residuals, and the median is unaffected by values at the tails. \n",
    "- the mean squared error can be highly sensitive to outliers, and mean absolute error can be somewhat sensitive to outliers (although less so than the mean squared error).\n",
    "\n",
    "### Classification concepts\n",
    "- Recall: TP/TP+FN; Precision: TP/TP+FP\n",
    "- Recall: ability of a classification model to identify all relevant instances\n",
    "- Precision: ability of a classification model to return only relevant instances\n",
    "- F1 score: single metric that combines recall and precision using the harmonic mean\n",
    "- Receiver operating characteristic (ROC) curve: plots the true positive rate (TPR) versus the false positive rate (FPR) as a function of the model’s threshold for classifying a positive\n",
    "- Area under the curve (AUC): metric to calculate the overall performance of a classification model based on area under the ROC curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, xcol=[1,2,7,8,9,10,11,12,13,14,15,16,17], ycol=[3]):\n",
    "    dataset = dataset[dataset.age!=\"Unknown\"]\n",
    "    dataset[\"gender\"].replace([\"female\",\"male\"],[0,1],inplace=True)\n",
    "    dataset = dataset.join(pd.get_dummies(dataset['casualty_class']).astype(int))\n",
    "    dataset = pd.concat([dataset, pd.get_dummies(dataset['travel']).astype(int)], axis=1)\n",
    "    dataset_x = dataset.iloc[:, xcol].values\n",
    "    dataset_y = dataset.iloc[:, ycol].values.astype(int)\n",
    "    return dataset_x, dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/generic.py:5890: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train = pd.read_csv('casualty_train.csv')\n",
    "test = pd.read_csv('casualty_test.csv')\n",
    "\n",
    "train_x, train_y = preprocess(train)\n",
    "test_x, test_y = preprocess(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8760616835860493\n",
      "(0.8760616835860493, 0.8760616835860493)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to tuple.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ce1aeec95fbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmeasurements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmeasurements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision: {:.2f}, recall: {:.2f}, fscore: {:.2f}, support: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasurements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to tuple.__format__"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_x,train_y)\n",
    "y_pred = knn.predict(test_x)\n",
    "\n",
    "# sore function gives the acuracy\n",
    "print(knn.score(test_x,test_y))\n",
    "\n",
    "# use library function for all things\n",
    "measurements = metrics.precision_recall_fscore_support(test_y, y_pred, beta=0.5, average='micro')\n",
    "print (measurements[1:3])\n",
    "print(\"precision: {:.2f}, recall: {:.2f}, fscore: {:.2f}, support: {:.2f}\".format(measurements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
